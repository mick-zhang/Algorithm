{"cells":[{"cell_type":"markdown","metadata":{"id":"imK1M524ln3O"},"source":["# SETUP\n","\n","Run these to create the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pk1n6zuIe3aC"},"outputs":[],"source":["!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz\n","!tar xf spark-3.2.2-bin-hadoop2.7.tgz\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCV3jcRJfSGo"},"outputs":[],"source":["import os\n","import findspark\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop2.7\"\n","\n","findspark.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCiQ_Dpzmtc1"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession\n","        .builder\n","        .appName(\"programming\")\n","        .master(\"local\")\n","        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\")\n","        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n","        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n","        .config('spark.ui.port', '4050')\n","        .getOrCreate()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRqwL5s92dmt"},"outputs":[],"source":["string_20210609 = '''worked_date,employee_id,delete_flag,hours_worked\n","2021-06-09,1001,N,7\n","2021-06-09,1002,N,3.75\n","2021-06-09,1003,N,7.5\n","2021-06-09,1004,N,6.25'''\n","\n","string_20210610 = '''worked_date,employee_id,delete_flag,hours_worked\n","2021-06-10,1001,N,8\n","2021-06-10,1002,N,6\n","2021-06-10,1003,N,1\n","2021-06-10,1004,N,10'''\n","\n","string_20210611 = '''worked_date,employee_id,delete_flag,hours_worked\n","2021-06-11,1001,N,6\n","2021-06-11,1002,N,7\n","2021-06-11,1003,N,9\n","2021-06-11,1004,N,5\n","2021-06-10,1003,Y,1\n","2021-06-10,1004,N,8'''\n","\n","rdd_20210609 = spark.sparkContext.parallelize(string_20210609.split('\\n'))\n","rdd_20210610 = spark.sparkContext.parallelize(string_20210610.split('\\n'))\n","rdd_20210611 = spark.sparkContext.parallelize(string_20210611.split('\\n'))"]},{"cell_type":"markdown","metadata":{"id":"bfCs8oOG2dmu"},"source":["# START HERE"]},{"cell_type":"markdown","metadata":{"id":"s8GrFPTf2dmu"},"source":["## Data Load\n","\n","Load each file consecutively, simulating a daily file feed.  Meaning, only one file should be read/loaded at a time because it is an iterative process.\n","\n","Rules:\n","\n","- A worker can only work once per day.\n","- delete_flag = Y means the record should be removed from the table, if exists.\n"]},{"cell_type":"markdown","metadata":{"id":"65SeX4S_2dmv"},"source":["Create the table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTfVXa3d2dmw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664298618471,"user_tz":240,"elapsed":19860,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"8c1fb518-049b-486f-ebf6-6003ccd0d983"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[]"]},"metadata":{},"execution_count":5}],"source":["# FILES WILL SHOW UP ON THE LEFT UNDER THE FOLDER ICON IF YOU WANT TO BROWSE THEM\n","OUTPUT_DELTA_PATH = '/content/output/delta/worked_hours'\n","\n","spark.sql('CREATE DATABASE IF NOT EXISTS EXERCISE')\n","\n","spark.sql('''\n","    CREATE TABLE IF NOT EXISTS EXERCISE.WORKED_HOURS(\n","        worked_date date\n","        , employee_id int\n","        , delete_flag string\n","        , hours_worked double\n","    ) USING DELTA\n","    PARTITIONED BY (worked_date)\n","    LOCATION \"{0}\"\n","    '''.format(OUTPUT_DELTA_PATH)\n",")"]},{"cell_type":"markdown","metadata":{"id":"QFzsE5NP2dmx"},"source":["Load 2021-06-09 dataframe and output table contents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ac5iWH752dmx"},"outputs":[],"source":["dataframe_20210609 = spark.read.csv(path = rdd_20210609, header=True)\n","# TODO Load data to table"]},{"cell_type":"markdown","metadata":{"id":"c_wInvwU2dmx"},"source":["Load 2021-06-10 dataframe and output table contents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4p5NEWSd2dmy"},"outputs":[],"source":["dataframe_20210610 = spark.read.csv(path = rdd_20210610, header=True)\n","# TODO Load data to table"]},{"cell_type":"markdown","metadata":{"id":"vjCsQon52dmy"},"source":["Load 2021-06-11 dataframe and output table contents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpzZlISc2dmy"},"outputs":[],"source":["dataframe_20210611 = spark.read.csv(path = rdd_20210611, header=True)\n","# TODO Load data to table"]},{"cell_type":"markdown","metadata":{"id":"dPHHpTSe3QAI"},"source":["## Data Analysis\n","\n","Using the table that was loaded, answer the below questions using either SQL or dataframe API."]},{"cell_type":"markdown","metadata":{"id":"62wWQ5DB3-NV"},"source":["1. How many hours does each employee average?"]},{"cell_type":"markdown","metadata":{"id":"cLEUUnyY4Kpy"},"source":["2. Which day has the least amount of hours worked?"]},{"cell_type":"markdown","metadata":{"id":"jYL4Z9RS4f8o"},"source":["3. Which day did each employee work their least amount of hours?"]},{"cell_type":"markdown","source":["# Data Load (Method 1: Database)"],"metadata":{"id":"uDHjdz18Sf91"}},{"cell_type":"markdown","source":["- Historical data will be exported as table \"T1\" to DB\n","- Refresh data will be exported to \"data\" folder\n","- On Refresh Loads (daily), \"T1\" from DB will used as historical data\n","- Refresh file will have timestamp on the file names\n","- Refresh file will be moved to \"data_archive\" folder after processing is complete\n","<br>\n","<br>\n","\n","**All future refresh data should be uploaded to \"data\" folder**\n","<br>\n","<br>\n","**Note**: Duplicate \"worked_date\" and \"employee_id\" without \"duplicate_flag\" will update the \"hours_worked\" field -- The idea here is that new record was added to update previous \"hours_worked\" since workers can only work once daily"],"metadata":{"id":"V1p6VyO7SkRZ"}},{"cell_type":"markdown","source":["### Inital Load (Execute one time only)"],"metadata":{"id":"3oWUvyJrSjc8"}},{"cell_type":"code","source":["import pandas as pd\n","import shutil\n","import glob\n","import os\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, col, concat, lead, desc, to_date\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","dataframe_20210609 = spark.read.csv(path = rdd_20210609, header=True)\n","dataframe_20210609.write.csv('/content/spark-warehouse/data/Refresh_20210609_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)\n","\n","dataframe_20210610 = spark.read.csv(path = rdd_20210610, header=True)\n","dataframe_20210610.write.csv('/content/spark-warehouse/data/Refresh_20210610_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)\n","\n","dataframe_20210611 = spark.read.csv(path = rdd_20210611, header=True)\n","dataframe_20210611.write.csv('/content/spark-warehouse/data/Refresh_20210611_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)\n","\n","\n","def process_data(df_historical):\n","\n","  df_new = df_historical.filter(df_historical.delete_flag == 'N')\n","  df_new.show()\n","  df_new.write.saveAsTable(\"T1\")\n","\n","\n","def move_file(source_folder, destination_folder, file_name):\n","  \n","  # Move refresh file to data_archive folder\n","  shutil.move(os.path.join(source_folder, file_name), destination_folder)\n","\n","\n","def main():\n","\n","  # Input \n","  historical_file = min(glob.glob('/content/spark-warehouse/data/*'), key=os.path.getctime)\n","  # Directory\n","  source_folder = '/content/spark-warehouse/data'\n","  destination_folder = '/content/spark-warehouse/data_archive'\n","    \n","  # Load Data\n","  df_historical = spark.read.csv(historical_file, header=True) # Load Historical Data\n","  \n","  # Transform Data\n","  process_data(df_historical)\n","  \n","  # Move File\n","  move_file(source_folder, destination_folder, historical_file)\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29OdMhe4SqY4","executionInfo":{"status":"ok","timestamp":1664298624145,"user_tz":240,"elapsed":4044,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"a87402d8-355a-4edb-a6c1-ba1812bbf62d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","+-----------+-----------+-----------+------------+\n","\n"]}]},{"cell_type":"markdown","source":["### Refresh Load (Daily)"],"metadata":{"id":"95cMTm5QSjNX"}},{"cell_type":"code","source":["import pandas as pd\n","import shutil\n","import glob\n","import os\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, col, concat, lead, desc, to_date\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","def process_data(df_refresh):\n","\n","  df_historical = spark.sql(\"SELECT * FROM T1\")\n","  df_refresh = df_refresh.filter(df_refresh.delete_flag == 'N')\n","\n","  df_new = (df_historical.unionByName(df_refresh) # Union the dfs\n","            .withColumn('CurrentHours_Worked', lead('hours_worked').over(Window.partitionBy('employee_id','worked_date').orderBy('employee_id',desc(to_date('worked_date'))))) # Create column comparing consecutive hours_worked\n","            .where(col('CurrentHours_Worked').isNull()) # retain last hours_worked by dropping null\n","            .drop('CurrentHours_Worked') # Drop the temp column\n","      ).orderBy(col('worked_date'), col('employee_id'))\n","\n","  df_new.show()\n","  df_new.write.mode(\"overwrite\").saveAsTable(\"Temp_Table\")\n","  df_temp = spark.sql(\"SELECT * FROM Temp_Table\")\n","  df_temp.write.mode(\"overwrite\").saveAsTable(\"T1\")\n","\n","\n","def move_file(source_folder, destination_folder, file_name):\n","  \n","  # Move refresh file to data_archive folder\n","  shutil.move(os.path.join(source_folder, file_name), destination_folder)\n","\n","\n","def main():\n","\n","  # Input\n","  refresh_file = min(glob.glob('/content/spark-warehouse/data/*'), key=os.path.getctime) # Input\n","  # Directory\n","  source_folder = '/content/spark-warehouse/data'\n","  destination_folder = '/content/spark-warehouse/data_archive'\n","\n","  # Load Data  \n","  df_refresh = spark.read.csv(refresh_file, header=True) # Load Refresh Data\n","\n","  if df_refresh.count() != 0:\n","    # Transform Data\n","    process_data(df_refresh)\n","    \n","    # Move File\n","    move_file(source_folder, destination_folder, refresh_file)\n","\n","  elif df_refresh.count() == 0:\n","    move_file(source_folder, destination_folder, refresh_file)\n","    print(\"No content in refresh_file\")\n","\n","\n","if __name__ == \"__main__\":\n","  \n","  refresh_source_folder = '/content/spark-warehouse/data'\n","  path, dirs, files = next(os.walk(refresh_source_folder))\n","\n","  if len(dirs) == 0:\n","    print('No refresh_file to process')\n","\n","  while len(dirs) > 0:\n","    main()\n","    path, dirs, files = next(os.walk(refresh_source_folder))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKB4itoDSlA6","executionInfo":{"status":"ok","timestamp":1664298629034,"user_tz":240,"elapsed":4900,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"1f01e66b-34a2-4a3e-902d-4b090f09148e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|          10|\n","+-----------+-----------+-----------+------------+\n","\n","+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|           8|\n","| 2021-06-11|       1001|          N|           6|\n","| 2021-06-11|       1002|          N|           7|\n","| 2021-06-11|       1003|          N|           9|\n","| 2021-06-11|       1004|          N|           5|\n","+-----------+-----------+-----------+------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Data Load (Method 2: Data Lake -- No Database)"],"metadata":{"id":"swEK3rgi12Rw"}},{"cell_type":"markdown","source":["- Historical file will be exported to \"output\" folder\n","- Refresh data will be exported to \"data\" folder\n","- On Refresh Loads (daily), existing historical file will be moved to \"output_archive\" folder after processing; updated historical file (historical + refresh) will be exported to \"output\" folder, which will be ready to use on the next Refresh Load\n","- Both Historical and Refresh file will have timestamp on the file names\n","- Refresh file will be moved to \"data_archive\" folder after processing is complete\n","<br>\n","<br>\n","\n","**All future refresh data should be uploaded to \"data\" folder**\n","<br>\n","<br>\n","**Note**: Duplicate \"worked_date\" and \"employee_id\" without \"duplicate_flag\" will update the \"hours_worked\" field -- The idea here is that new record was added to update previous \"hours_worked\" since workers can only work once daily"],"metadata":{"id":"Ynr906cH12B3"}},{"cell_type":"markdown","source":["### Initial Load (Execute one time only)"],"metadata":{"id":"yrhJgHXs11ut"}},{"cell_type":"code","source":["import pandas as pd\n","import shutil\n","import glob\n","import os\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, col, concat, lead, desc, to_date\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","# FIRST RUN -- Export historical data to output folder\n","dataframe_20210609 = spark.read.csv(path = rdd_20210609, header=True)\n","dataframe_20210609.write.csv('/content/output/historical_20210609_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)\n","# FIRST RUN -- Export refresh data to data folder\n","dataframe_20210610 = spark.read.csv(path = rdd_20210610, header=True)\n","dataframe_20210610.write.csv('/content/data/refresh_20210610_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)\n","dataframe_20210611 = spark.read.csv(path = rdd_20210611, header=True)\n","dataframe_20210611.write.csv('/content/data/refresh_20210611_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")), header=True)"],"metadata":{"id":"TUCFKAOQ1z9K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Refresh Load (Daily)"],"metadata":{"id":"HVI5zctJ3Y7R"}},{"cell_type":"code","source":["import pandas as pd\n","import shutil\n","import glob\n","import os\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, col, concat, lead, desc, to_date\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def process_data(df_historical, df_refresh):\n","\n","  # Union Data\n","  df_new = df_historical.union(df_refresh)\n","\n","  # Add Index column\n","  w = Window.orderBy('worked_date')\n","  df_new = df_new.withColumn('index', row_number().over(w))\n","\n","  # Assign row_number by using windows function\n","  windowSpec  = Window.partitionBy(col(\"worked_date\"),\n","                                  col(\"employee_id\"),\n","                                  col('delete_flag')) \\\n","                      .orderBy(col(\"index\").desc())\n","  df_new = df_new.withColumn(\"row_number\",row_number().over(windowSpec))\n","\n","  # Filter rows\n","  df_new = df_new.filter((df_new.row_number == 1) & (df_new.delete_flag == 'N'))\n","\n","  # Filter columns\n","  columns = ['worked_date', 'employee_id', 'delete_flag', 'hours_worked']\n","  df_new = df_new.select(columns)\n","    \n","  return df_new\n","\n","\n","def move_historical_file(historical_source_folder, historical_destination_folder):\n","  \n","  # Move existing historical file to output_archive folder\n","  file_names = os.listdir(historical_source_folder)\n","  for i in file_names:\n","    shutil.move(os.path.join(historical_source_folder, i), historical_destination_folder)\n","\n","\n","def move_refresh_file(refresh_source_folder, refresh_destination_folder, refresh_file):\n","  \n","  # Move refresh file to data_archive folder\n","  shutil.move(os.path.join(refresh_source_folder, refresh_file), refresh_destination_folder)\n","\n","\n","def main():\n","\n","  # Input\n","  historical_file = max(glob.glob('/content/output/*'), key=os.path.getctime)\n","  refresh_file = min(glob.glob('/content/data/*'), key=os.path.getctime)\n","  # Output\n","  historical_output = '/content/output/historical_{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\"))\n","  # Directory\n","  historical_source_folder = '/content/output'\n","  historical_destination_folder = '/content/output_archive'\n","  refresh_source_folder = '/content/data'\n","  refresh_destination_folder = '/content/data_archive'\n","  \n","  # Load Data\n","  df_historical = spark.read.csv(historical_file, header=True)\n","  df_refresh = spark.read.csv(refresh_file, header=True)\n","  \n","  if df_refresh.count() != 0:\n","    # Transform Data\n","    df_new = process_data(df_historical, df_refresh)\n","      \n","    # Cache Data\n","    df_new = df_new.cache()\n","    df_new.show()\n","\n","    # Move file\n","    move_historical_file(historical_source_folder, historical_destination_folder)\n","    move_refresh_file(refresh_source_folder, refresh_destination_folder, refresh_file)\n","\n","    # Export updated historical file to output folder\n","    df_new.write.csv(historical_output, header=True)\n","\n","  elif df_refresh.count() == 0:\n","    move_refresh_file(refresh_source_folder, refresh_destination_folder, refresh_file)\n","    print(\"No content in refresh_file\")\n","\n","\n","if __name__ == \"__main__\":\n","  \n","  refresh_source_folder = '/content/data'\n","  path, dirs, files = next(os.walk(refresh_source_folder))\n","\n","  if len(dirs) == 0:\n","    print('No refresh_file to process')\n","\n","  while len(dirs) > 0:\n","    main()\n","    path, dirs, files = next(os.walk(refresh_source_folder))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnN0eXC2-zmX","executionInfo":{"status":"ok","timestamp":1664298633504,"user_tz":240,"elapsed":3099,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"aa1f24fb-5f6f-4877-9909-fd13a79eba08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|          10|\n","+-----------+-----------+-----------+------------+\n","\n","+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|           8|\n","| 2021-06-11|       1001|          N|           6|\n","| 2021-06-11|       1002|          N|           7|\n","| 2021-06-11|       1003|          N|           9|\n","| 2021-06-11|       1004|          N|           5|\n","+-----------+-----------+-----------+------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Data Analysis"],"metadata":{"id":"uu-c18IO8JT8"}},{"cell_type":"markdown","source":["##### 1. How many hours does each employee average?"],"metadata":{"id":"GRupRwy3E02o"}},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM T1\").show()\n","\n","spark.sql('''\n","SELECT \n","employee_id\n",",AVG(hours_worked) as average_hours_worked\n","FROM T1\n","GROUP BY 1\n","ORDER BY 1\n","''').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvg6_96U6HOY","executionInfo":{"status":"ok","timestamp":1664301496352,"user_tz":240,"elapsed":767,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"2043c45e-a154-4114-a663-ec91d814d4f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|           8|\n","| 2021-06-11|       1001|          N|           6|\n","| 2021-06-11|       1002|          N|           7|\n","| 2021-06-11|       1003|          N|           9|\n","| 2021-06-11|       1004|          N|           5|\n","+-----------+-----------+-----------+------------+\n","\n","+-----------+--------------------+\n","|employee_id|average_hours_worked|\n","+-----------+--------------------+\n","|       1001|                 7.0|\n","|       1002|   5.583333333333333|\n","|       1003|   5.833333333333333|\n","|       1004|   6.416666666666667|\n","+-----------+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["##### 2. Which day has the least amount of hours worked?"],"metadata":{"id":"03TtyLWkFCsQ"}},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM T1\").show()\n","\n","spark.sql('''\n","SELECT \n","worked_date\n",",SUM(hours_worked) AS total_hours_worked\n","FROM T1\n","GROUP BY 1\n","ORDER BY 2 \n","LIMIT 1\n","''').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgHSpDGoFFC_","executionInfo":{"status":"ok","timestamp":1664301894491,"user_tz":240,"elapsed":727,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"4a68d008-c756-4f95-c222-0f3bad72384e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|           8|\n","| 2021-06-11|       1001|          N|           6|\n","| 2021-06-11|       1002|          N|           7|\n","| 2021-06-11|       1003|          N|           9|\n","| 2021-06-11|       1004|          N|           5|\n","+-----------+-----------+-----------+------------+\n","\n","+-----------+------------------+\n","|worked_date|total_hours_worked|\n","+-----------+------------------+\n","| 2021-06-10|              23.0|\n","+-----------+------------------+\n","\n"]}]},{"cell_type":"markdown","source":["##### 3. Which day did each employee work their least amount of hours?"],"metadata":{"id":"LM9RWqBBFGp-"}},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM T1\").show()\n","\n","spark.sql('''\n","SELECT \n","a.worked_date\n",",a.employee_id\n",",b.minimum_hours_worked\n","FROM T1 a\n","RIGHT JOIN\n","(\n","SELECT \n","employee_id\n",",MIN(hours_worked) as minimum_hours_worked\n","FROM T1\n","GROUP BY 1\n",") b\n","ON a.employee_id = b.employee_id\n","AND a.hours_worked = b.minimum_hours_worked\n","''').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSuZJefvFIdv","executionInfo":{"status":"ok","timestamp":1664302369592,"user_tz":240,"elapsed":679,"user":{"displayName":"Mick Zhang","userId":"12814602485392450081"}},"outputId":"c3450afa-5eea-4ccb-f149-6ea2fc3f7700"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+------------+\n","|worked_date|employee_id|delete_flag|hours_worked|\n","+-----------+-----------+-----------+------------+\n","| 2021-06-09|       1001|          N|           7|\n","| 2021-06-09|       1002|          N|        3.75|\n","| 2021-06-09|       1003|          N|         7.5|\n","| 2021-06-09|       1004|          N|        6.25|\n","| 2021-06-10|       1001|          N|           8|\n","| 2021-06-10|       1002|          N|           6|\n","| 2021-06-10|       1003|          N|           1|\n","| 2021-06-10|       1004|          N|           8|\n","| 2021-06-11|       1001|          N|           6|\n","| 2021-06-11|       1002|          N|           7|\n","| 2021-06-11|       1003|          N|           9|\n","| 2021-06-11|       1004|          N|           5|\n","+-----------+-----------+-----------+------------+\n","\n","+-----------+-----------+--------------------+\n","|worked_date|employee_id|minimum_hours_worked|\n","+-----------+-----------+--------------------+\n","| 2021-06-11|       1001|                   6|\n","| 2021-06-09|       1002|                3.75|\n","| 2021-06-10|       1003|                   1|\n","| 2021-06-11|       1004|                   5|\n","+-----------+-----------+--------------------+\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1qGkqeVpLgfYCeLiCpFuvtkurBWNWqqJJ","timestamp":1664236633173}],"collapsed_sections":["imK1M524ln3O","s8GrFPTf2dmu","dPHHpTSe3QAI","3oWUvyJrSjc8","swEK3rgi12Rw","yrhJgHXs11ut","HVI5zctJ3Y7R","uu-c18IO8JT8","GRupRwy3E02o","03TtyLWkFCsQ","LM9RWqBBFGp-"]},"kernelspec":{"display_name":"Python 3.9.13 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"4530bbb8a57becea6488ec4a808b18ebf01125d6ca506583ca77ab9262696528"}}},"nbformat":4,"nbformat_minor":0}